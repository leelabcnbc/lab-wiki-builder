{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention based models\n",
    "\n",
    "K. Xu, J. L. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. S. Zemel, and Y. Bengio, “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention,” arXiv, vol. cs.LG, Feb. 2015.\n",
    "\n",
    "Notes: Great application of \"attention\" to image captioning.\n",
    "\n",
    "Notice that where to attend is not supervised, just like the case in CTC.\n",
    "\n",
    "CS231n says that one problem with this work is the resolution of attention. Maybe can be addressed using per pixel hypercolumn features, after upsampling. CS231n's solution is to use Spatial Transformation Networks.\n",
    "\n",
    "pp. 4.\n",
    "\n",
    "Check CS231n for the notation, notice that I think here T should be $T_{D+m+n, 4n}$.\n",
    "\n",
    "Section 4.1 Hard attention.\n",
    "\n",
    "This version is learned by maximizing lower bound. In Eq. (11), s is a vector of all choices along time. and to learn this, we need sampling.\n",
    "\n",
    "in pp.5, there are some further refinements on optimization. Maybe these are just some tricks in MCMC. anyway.\n",
    "\n",
    "Section 4.2 soft attention.\n",
    "\n",
    "pp. 6\n",
    "\n",
    "most of left column is about connection between soft and hard versions. Ignored.\n",
    "\n",
    "\n",
    "Section 4.2.1 Doubly Stochastic Attention.\n",
    "\n",
    "No idea why across time, the alpha should sum to 1 for each location. See <https://github.com/kelvinxu/arctic-captions/issues/38> and <https://www.reddit.com/r/MachineLearning/comments/2vjdcw/show_attend_and_tell_neural_image_caption/cok4yc9/>. Maybe it just worked, whatsoever.\n",
    "\n",
    "Indeed the author says it doesn't matter in practice. Check <https://github.com/kelvinxu/arctic-captions/issues/38#issuecomment-282103028>\n",
    "\n",
    "In addition, that $\\beta$ stuff might be some hack that emprically helps.\n",
    "\n",
    "~~~\n",
    "@article{Xu:2015ut,\n",
    "author = {Xu, Kevin and Ba, Jimmy Lei and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard S and Bengio, Yoshua},\n",
    "title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},\n",
    "journal = {ArXiv e-prints},\n",
    "year = {2015},\n",
    "volume = {cs.LG},\n",
    "month = feb,\n",
    "annote = {Great application of \"attention\" to image captioning.\n",
    "\n",
    "Notice that where to attend is not supervised, just like the case in CTC.\n",
    "\n",
    "CS231n says that one problem with this work is the resolution of attention. Maybe can be addressed using per pixel hypercolumn features, after upsampling.\n",
    "\n",
    "pp. 4.\n",
    "\n",
    "Check CS231n for the notation, notice that I think here T should be $T_{D+m+n, 4n}$.\n",
    "\n",
    "Section 4.1 Hard attention.\n",
    "\n",
    "This version is learned by maximizing lower bound. In Eq. (11), s is a vector of all choices along time. and to learn this, we need sampling.\n",
    "\n",
    "in pp.5, there are some further refinements on optimization. Maybe these are just some tricks in MCMC. anyway.\n",
    "\n",
    "Section 4.2 soft attention.\n",
    "\n",
    "pp. 6\n",
    "\n",
    "most of left column is about connection between soft and hard versions. Ignored.\n",
    "\n",
    "\n",
    "Section 4.2.1 Doubly Stochastic Attention.\n",
    "\n",
    "No idea why across time, the alpha should sum to 1 for each location. See <https://github.com/kelvinxu/arctic-captions/issues/38> and <https://www.reddit.com/r/MachineLearning/comments/2vjdcw/show_attend_and_tell_neural_image_caption/cok4yc9/>. Maybe it just worked, whatsoever.\n",
    "\n",
    "In addition, that \\beta stuff might be some hack that emprically helps.\n",
    "\n",
    "},\n",
    "keywords = {deep learning, recurrent},\n",
    "read = {Yes},\n",
    "rating = {5},\n",
    "date-added = {2017-02-21T16:11:00GMT},\n",
    "date-modified = {2017-02-23T19:57:09GMT},\n",
    "url = {http://arxiv.org/abs/1502.03044},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2015/Xu/arXiv%202015%20Xu.pdf},\n",
    "file = {{arXiv 2015 Xu.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2015/Xu/arXiv 2015 Xu.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/uuid/881D0A60-2E59-477D-814F-5E8908F34D89}}\n",
    "}\n",
    "~~~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
